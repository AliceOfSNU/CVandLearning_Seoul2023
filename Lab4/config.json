{
    "beam_width" : 3,
    "lr"         : 5e-4,
    "lr_schedule": "ReduceLROnPlateau with 2 patience, by 0.5, only after tf has been fully reduced",
    "epochs"     : 100,
    "batch_size": 96,
    "weight_decay" : 1e-3,
    "encoder_cnn_kernel": 3,
    "encoder_cnn_layers": 6,
    "encoder_cnn_channels": [64], 
    "dropout": 0.2,
    "n_decodes": 10,
    "mlp_layers": 2,
    "tf_schedule": "Reduce linearly from 0.9->0.6 in 30 epoch",
    "encoder_hidden_size": 64,
    "attn_hidden_size": 128,
    "decoder_hidden_size": 128,
    "data_features": 28,
    "init_method": "uniform -0.1~0.1",
    "run_id": "attn_64_greedy_training",
    "notes": "greedy decoding for train",
    "dataset": "E2EASR_kaggle/train-clean-100",
    "seed":12
}