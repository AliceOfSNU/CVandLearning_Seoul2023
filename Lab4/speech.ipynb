{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "CMUdict_ARPAbet = {\n",
    "    \"\" : \" \",\n",
    "    \"[SIL]\": \"-\", \"NG\": \"G\", \"F\" : \"f\", \"M\" : \"m\", \"AE\": \"@\",\n",
    "    \"R\"    : \"r\", \"UW\": \"u\", \"N\" : \"n\", \"IY\": \"i\", \"AW\": \"W\",\n",
    "    \"V\"    : \"v\", \"UH\": \"U\", \"OW\": \"o\", \"AA\": \"a\", \"ER\": \"R\",\n",
    "    \"HH\"   : \"h\", \"Z\" : \"z\", \"K\" : \"k\", \"CH\": \"C\", \"W\" : \"w\",\n",
    "    \"EY\"   : \"e\", \"ZH\": \"Z\", \"T\" : \"t\", \"EH\": \"E\", \"Y\" : \"y\",\n",
    "    \"AH\"   : \"A\", \"B\" : \"b\", \"P\" : \"p\", \"TH\": \"T\", \"DH\": \"D\",\n",
    "    \"AO\"   : \"c\", \"G\" : \"g\", \"L\" : \"l\", \"JH\": \"j\", \"OY\": \"O\",\n",
    "    \"SH\"   : \"S\", \"D\" : \"d\", \"AY\": \"Y\", \"S\" : \"s\", \"IH\": \"I\",\n",
    "    \"[SOS]\": \"[SOS]\", \"[EOS]\": \"[EOS]\"\n",
    "}\n",
    "\n",
    "CMUdict = list(CMUdict_ARPAbet.keys())\n",
    "ARPAbet = list(CMUdict_ARPAbet.values())\n",
    "\n",
    "PHONEMES = CMUdict[:-2]\n",
    "LABELS = ARPAbet[:-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alice/anaconda3/envs/lowergcc/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create dataset from data  ../../data/train-clean-100\n",
      "\ttotal mfcc cnt:  28539\n",
      "\ttotal transcript cnt:  28539\n",
      "create dataset from data  ../../data/dev-clean\n",
      "\ttotal mfcc cnt:  2703\n",
      "\ttotal transcript cnt:  2703\n"
     ]
    }
   ],
   "source": [
    "import dataset\n",
    "from dataset import AudioDataset\n",
    "\n",
    "# probably want to collect some RAM before we go.\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "train_data = AudioDataset('train-clean-100')\n",
    "val_data = AudioDataset('dev-clean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(830, 28) float32\n",
      "[63 65 78 68 32 84 72 69 32 80 79 79 82 32 83 73 76 76 89 32 84 72 73 78\n",
      " 71 83 32 82 85 70 70 76 69 68 32 85 80 32 84 72 69 73 82 32 70 69 65 84\n",
      " 72 69 82 83 32 65 78 68 32 76 79 79 75 69 68 32 77 73 83 69 82 65 66 76\n",
      " 69 32 65 83 32 79 78 76 89 32 65 32 76 73 84 84 76 69 32 66 73 82 68 32\n",
      " 67 65 78 32 76 79 79 75 32 87 72 69 78 32 73 84 32 73 83 32 85 78 72 65\n",
      " 80 80 89 64]\n",
      "(684, 28) float32\n",
      "[63 72 69 32 84 72 79 85 71 72 84 32 73 84 32 87 65 83 32 65 32 76 65 83\n",
      " 84 32 66 85 82 83 84 32 79 70 32 69 78 69 82 71 89 32 72 69 32 75 78 69\n",
      " 87 32 72 79 87 32 67 76 79 83 69 32 84 72 69 89 32 66 79 84 72 32 87 69\n",
      " 82 69 32 84 79 32 69 88 72 65 85 83 84 73 79 78 64]\n"
     ]
    }
   ],
   "source": [
    "mfcc,  transcript = val_data[0]\n",
    "print(mfcc.shape, mfcc.dtype)\n",
    "print(transcript)\n",
    "\n",
    "mfcc,  transcript = val_data[1]\n",
    "print(mfcc.shape, mfcc.dtype)\n",
    "print(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset samples = 28539, batches = 445\n",
      "torch.Size([64, 1683, 28]) torch.Size([64, 287]) torch.Size([64]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 64\n",
    "train_loader =  DataLoader(\n",
    "            train_data,\n",
    "            batch_size=batch_size,\n",
    "            drop_last=True,\n",
    "            shuffle=True,\n",
    "            collate_fn=train_data.collate_fn\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
    "\n",
    "# sanity check\n",
    "for data in train_loader:\n",
    "    x, y, lx, ly = data\n",
    "    print(x.shape, y.shape, lx.shape, ly.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1671, 28]) torch.Size([64, 269]) torch.Size([64]) torch.Size([64])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ASRModel' object has no attribute 'decoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m x, y, lx, ly \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape, y\u001b[38;5;241m.\u001b[39mshape, lx\u001b[38;5;241m.\u001b[39mshape, ly\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 12\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Bartons/Vision/CV2024/CVandLearning_Seoul2023/Lab4/model.py:95\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, x, lengths_x)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, lengths_x):\n\u001b[1;32m     91\u001b[0m \n\u001b[1;32m     92\u001b[0m     \u001b[38;5;66;03m#if self.training:\u001b[39;00m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;66;03m#    x = self.augmentations(x)\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m     encoder_out, encoder_lens   \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x, lengths_x)\n\u001b[1;32m     96\u001b[0m     decoder_out                 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(encoder_out)\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoder_out, encoder_lens\n",
      "File \u001b[0;32m~/anaconda3/envs/lowergcc/lib/python3.9/site-packages/torch/nn/modules/module.py:1269\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1267\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1268\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1269\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ASRModel' object has no attribute 'decoder'"
     ]
    }
   ],
   "source": [
    "from model import ASRModel\n",
    "from defines import PHONEMES\n",
    "\n",
    "model = ASRModel(\n",
    "    input_size = 28, \n",
    "    embed_size= 64,\n",
    "    output_size = len(PHONEMES)\n",
    ")\n",
    "for data in train_loader:\n",
    "    x, y, lx, ly = data\n",
    "    print(x.shape, y.shape, lx.shape, ly.shape)\n",
    "    model.forward(x, lx)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lowergcc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
